{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste errors into variable `error_message` below, then run, to prettify somewhat. Proof of concept :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"seq2seq_attention.py\", line 371, in <module>\n",
      "    enc_out=enc_out, prev_dec_state=dec_state)\n",
      "  File \"seq2seq_attention.py\", line 238, in forward\n",
      "    enc_out_U = enc_out_U.view(seq_len, batch_size, hidden_size)\n",
      "RuntimeError: size '[5 x 4 x 5]' is invalid for input of with 200 elements at /Users/soumith/miniconda2/conda-bld/pytorch_1493757886437/work/torch/lib/TH/THStorage.c:59\n"
     ]
    }
   ],
   "source": [
    "error_message = \"\"\"\n",
    "0 {'input': \"I'm\", 'target': 'Je '}\n",
    "1 {'input': 'Whe', 'target': 'Qua'}\n",
    "2 {'input': 'Som', 'target': 'Que'}\n",
    "3 {'input': 'He ', 'target': 'Il '}\n",
    "vocab size 18\n",
    "encoder_batch.size() torch.Size([5, 4])\n",
    "decoder_batch.size() torch.Size([5, 4])\n",
    "Traceback (most recent call last):\n",
    "  File \"seq2seq_attention.py\", line 371, in <module>\n",
    "    enc_out=enc_out, prev_dec_state=dec_state)\n",
    "  File \"/Users/hugh2/conda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n",
    "    result = self.forward(*input, **kwargs)\n",
    "  File \"seq2seq_attention.py\", line 238, in forward\n",
    "    enc_out_U = enc_out_U.view(seq_len, batch_size, hidden_size)\n",
    "  File \"/Users/hugh2/conda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\", line 471, in view\n",
    "    return View(*sizes)(self)\n",
    "  File \"/Users/hugh2/conda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\", line 98, in forward\n",
    "    result = i.view(*self.sizes)\n",
    "RuntimeError: size '[5 x 4 x 5]' is invalid for input of with 200 elements at /Users/soumith/miniconda2/conda-bld/pytorch_1493757886437/work/torch/lib/TH/THStorage.c:59\"\"\"\n",
    "\n",
    "in_stacktrace = False\n",
    "left_user_modules = False\n",
    "message_lines = error_message.split('\\n')\n",
    "i = 0\n",
    "while i < len(message_lines):\n",
    "    line = message_lines[i]\n",
    "    if line.startswith('Traceback'):\n",
    "        in_stacktrace = True\n",
    "        i += 1\n",
    "        continue\n",
    "    if in_stacktrace:\n",
    "        if line.strip().startswith('File \"'):\n",
    "            if '/torch/nn/modules' in line:\n",
    "                i += 2\n",
    "                continue\n",
    "            if '/torch/autograd/' in line:\n",
    "                i += 2\n",
    "                continue\n",
    "#                 left_user_modules = True\n",
    "            print(line)\n",
    "            print(message_lines[i + 1])\n",
    "            i += 2\n",
    "            continue\n",
    "        else:\n",
    "            print(line)\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
